\documentclass{customization}

\usepackage{multicol}
\usepackage{comment}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{calc}
\usepackage{longtable}

\usepackage{listings}
\usepackage[most]{tcolorbox}
\usepackage{inconsolata}

% Code listing setup
\lstset{showstringspaces=false}
\definecolor{logocolor}{rgb}{0.11, 0.19, 0.33}
\newtcblisting[auto counter]{sexylisting}[2][]{sharp corners, 
    fonttitle=\bfseries, colframe=logocolor, listing only, 
    listing options={basicstyle=\ttfamily,language=java}, 
    title=Code example \thetcbcounter: #2, #1}


\title{Summer Student Project Report}

\begin{document}


%------------ Information ---------------------

\titre{AWS Cloud Computing\\ Introduction - Report}
\UE{ }
\sujet{\LaTeX Blabla}

\enseignant{Mateusz \textsc{Żbikowski} PhD}

\eleves{
Krzysztof \textsc{Wilczyński}
}

%----------- Initialization -------------------
        
\fairemarges 
\fairepagedegarde 

\begin{Large}
\end{Large}
\vspace{0.5cm}


%\tabledematieres  % Contents

%---------------- Body ------------------------


%==============================================
%               Introduction
%==============================================

\noindent
Please note that this document is a copy of the repository's overview page (\underline{\href{https://github.com/3sztof/cloud_computing}{LINK}}). Furthermore, the github documentation page will be updated more frequently and is likely to contain more practical information (such as links to the useful resources).

\section{Steps taken}

\begin{itemize}
    \item Applied for the AWS Educate financing program to get 40\$ cloud experiments financing
    \item Studied all of the resources provided by the lecturer in order to gain a basic theoretical knowledge about the vast possibilities of AWS (which gives us much more than just parallel computing)
    \item Installed Anaconda, AWS CMD Tools etc. on my Linux machine.
    \item Created several AWS EC2 (Amazon Web Services Elastic Coputer Cloud) machines in different availability zones just for fun - to test the possibilities of the nodes and their operating systems.
    \item Created Key Pairs allowing for remote connection to the EC2 instances (those files are not in the repository for safety reasons - there are bots scanning github for such keys to run unauthorized apps such as Bitcoin miners on someone else's machines, thus generating incredibly high costs).
    \item Connected to the EC2 instances using the following pattern (Linux): 
    \begin{sexylisting}[colback=white]{}
$ chmod 400 <private-key-path>/my-key-pair.pem
$ ssh -i <private-key-path>/my-key-pair.pem 
    <ec2-user>@<ec2-node-public-dns-adress>
    \end{sexylisting}
    Where:
    \begin{itemize}
        \item private-key-path is the directory on the local system containing the SSH private key generated when creating "Key Pairs" in the AWS EC2 console.
        \item ec2-user depends on the operating system running on the node (for Ubuntu linux, the login and password defaults to ubuntu:ubuntu)
        \item ec2-node-public-dns-adress is the public dns adress assigned to the node (look it up in the EC2 management console)
    \end{itemize}
    \item Deployed some simple helloworld / pi approximation scripts on a single EC2 machine (under native python shipped with Ubuntu Linux).
    \item Read the publication on UC Berkeley's Biostat statistical computing cluster software and made an attempt to deploy one at AWS using the instructions provided in the tutorial repository (see: Useful Resources section). The project was abandoned as too ambitious - I have no sufficient R language knowledge, it was decided to stick to Python.
    \item Made serveral attepmts, wasting a lot of time in order to deploy a High Performance Computing (HPC) cloud (see the next section for details)
    \item After failing to find any help regarding the HPC cloud deployment, a change of approach was needed - it was decided to go with the Apache Spark cloud environment deployed on three AWS EC2 machines: one "master" and two "worker" nodes.
    \item Configured the simple Apache Spark Cluster (release emr-5.20.0) according to the instructions provided in the "Useful Resources" section of this report.
    \item Installed the Apache spark client libraries (from \underline{\href{https://archive.apache.org/dist/spark/spark-2.2.0/}{here}}) within the Lunux host's PATH.
    \item Created the needed environmental variable:
    \begin{sexylisting}[colback=white]{}
HADOOP_CONF_DIR = 
    ~/<my-local-configuration-files-directory>
    \end{sexylisting}
    Where:
    \begin{itemize}
        \item my-local-configuration-files-directory is a path to a folder containing hadoop configuration files on my local Linux machine.
    \end{itemize}
    \item Created a SSH tunnel to the Apache Spark AWS cluster "master node" (one of the three EC2 machines that is in control of the "workers") using dynamic port forwarding as follows:
    \begin{sexylisting}[colback=white]{}
$ ssh -i <private-key-path>/mykeypair.pem -D <port> 
    -N hadoop@<spark-master-node-public-dns-adress>
    \end{sexylisting}
    Where:
    \begin{itemize}
        \item private-key-path is the directory on the local system containing the SSH private key generated when creating "Key Pairs" in the AWS EC2 console.
        \item port a local machine's port used to create the SSH tunnel
        \item spark-master-node-public-dns-adress is the public dns adress assigned to the spark cluster's master node (it can be checked in the AWS EC2 management console - it should usually be in a different network than the worker nodes).
        \item 
    \end{itemize}
    \item Deployed a sample job (PI number digits calculations based on Monte Carlo method) that I found online to the Apache Spark Cluster configured in the previous steps. The deployment is executed as follows (providing the spark library is present in local machine's PATH environment):
    \begin{sexylisting}[colback=white]{}
$ spark-submit --class <class> --master <master> 
    --deploy-mode <mode> --py-files <py-path> 
    <script> <args>
    \end{sexylisting}
    Where:
    \begin{itemize}
        \item class is the entry point of the application (main class)
        \item master is the adress URL of the Apache Spark cluster's master node (spark://spark-master-node-public-dns-adress)
        \item deploy-mode is set to either "cluster" or "client" depending on wheather the script should be started on the cluster's worker nodes or the local client.
        \item py-files is the search path for the python scripts and dependencies on the local machine (useful for complex python projects)
        \item script is the python scipt file (.py) to be executed
        \item args are the space-separated arguments that will be passed to the main method of the script specified in the previous step
    \end{itemize}

\end{itemize}


\section{Encountered problems}

\hspace{\parindent}
In the first attempts, many EC2 nodes were deployed in different availability zones in order to properly deploy a High Performance Computing (HPC) cluster. The clusters could only be deployed in US-West and US-East zones but despite trying out every region and many configuration files, all of the attempts were failures. The tutorial (\underline{\href{https://aws.amazon.com/hpc/sc15/getting-started/}{HPC tutorial}}) provided by AWS contained links to HPC configuration files that were not accesible (such as \underline{\href{https://s3.amazonaws.com/cfncluster-public-scripts/cfncluster-simple-cfd.cfn.json}{this}}). Other configuration JSONs proved to cause unrealistically high costs. Many hours were wasted trying to find a solution and eventually, the decision was to change the approach to AWS Apache Spark cluster.

\vspace{0.7cm}
\noindent
No financial help (\underline{\href{https://aws.amazon.com/education/awseducate/}{Amazon Educate}}) has been provided despite many attempts to fill in the forms and contacting the AWS support. This was a serious limitation, as the 40\$ grant could really help in deploying experimental clusters.

\vspace{0.7cm}
\noindent
A lot of time was wasted trying to access the cluster WEB services provided by Apache Spark on the master node using \underline{\href{https://chrome.google.com/webstore/detail/foxyproxy-standard/gcknhkkoolaabfmlnjonogaaifnjlfnp?hl=en}{FoxyProxy Chrome addon}}. In the end, this was not really needed...

\vspace{0.7cm}
\noindent
When I finally managed to set up an Apache Spark cluster, it was already a very busy time in the semester - all the other projects and exams made it impossible to continue the struggle and write something amazing - it is the plan for the future though! (Todo) :)

\section{Plans for the future}
\begin{enumerate}
    \item Try again to configure HPC and deploy a Python application there.
    \item Learn R statistics language and deploy the UC Berkeley Biostat cluster on AWS.
    \item Make a comparison between parallel computing on a single machine and a cluster of cores - the result should be different depending on the complexity of the calculations. It is expected that the computing will actually take longer on the cluster in case of simple and short task (as the nodes have to communicate via sockets - that takes time). On the other hand, the more complex task should be much faster using a cluster such as HPC as the computation can be dissipated between many more cores than in case of a single machine.
\end{enumerate}
 


























\begin{comment}

%-------------- Examples ----------------------

\newpage
\section{Examples}

Examples

%----------- Insert image ---------------------

\insererfigure{logos/cern.png}{3cm}{Caption}{Label}

% \begin{figure}[H]
%     	\centering
%     	\includegraphics[scale=0.1]{warsaw1.png}
%     	%\caption{Wykresy Bodego transmitancji G, HG oraz HGW}
%     	%\label{fig:transmitancja}
% \end{figure}

Cite the image number: \ref{fig: Label}


%------------- Equation ------------------------

\begin{equation} \label{eq: exemple}
\rho + \Delta = 42
\end{equation}

Equation number reference: \ref{eq: exemple}

% ---------- Math in text ----------------------

Blablabla \$ blablabla $\rho$. 

\end{comment}

\end{document}

